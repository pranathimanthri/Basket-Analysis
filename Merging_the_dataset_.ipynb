{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pPNCdlL81m36"},"outputs":[],"source":["from pyspark.sql import SparkSession\n"]},{"cell_type":"code","source":["# Import the necessary library\n","from pyspark.sql import SparkSession\n","\n","# Configure the SparkSession to use the local master\n","spark = (\n","    SparkSession.builder\n","        .appName(\"Instacart-Merge\")\n","        .master(\"local[*]\")\n","        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n","        .config(\"spark.ui.port\", \"4041\")\n","        .getOrCreate()\n",")"],"metadata":{"id":"TDHCFrBe2CeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read source files\n","aisles   = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/aisles.csv\")\n","depts    = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/departments.csv\")\n","products = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/products.csv\")\n","orders   = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/orders.csv\")\n","train_op = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/order_products__train.csv\")\n","prior_op = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/order_products__prior.csv\")\n"],"metadata":{"id":"SdbPLjAz2HD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rzl2lCfGnlyK","executionInfo":{"status":"ok","timestamp":1745444860717,"user_tz":360,"elapsed":18079,"user":{"displayName":"Murali Prateek Manthri","userId":"06912547824777002212"}},"outputId":"bd0ef482-0c7d-4701-f39c-270fcb3d06ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# quick peeks (head / shape analogues)\n","train_op.show(5, truncate=False)\n","print((\"train_op rows\", train_op.count()), (\"columns\", len(train_op.columns)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hy--4MaL2L3b","outputId":"21958627-ba6e-4564-9d4d-5ead3c118da4","executionInfo":{"status":"ok","timestamp":1745446912592,"user_tz":360,"elapsed":1861,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+----------+-----------------+---------+\n","|order_id|product_id|add_to_cart_order|reordered|\n","+--------+----------+-----------------+---------+\n","|1       |49302     |1                |1        |\n","|1       |11109     |2                |1        |\n","|1       |10246     |3                |0        |\n","|1       |49683     |4                |0        |\n","|1       |43633     |5                |1        |\n","+--------+----------+-----------------+---------+\n","only showing top 5 rows\n","\n","('train_op rows', 1384617) ('columns', 4)\n"]}]},{"cell_type":"code","source":["prior_op.show(5, truncate=False)\n","print((\"prior_op rows\", prior_op.count()), (\"columns\", len(prior_op.columns)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLlowHkb2N57","outputId":"fc2d17a5-3625-41c8-dfb8-68554b065b83","executionInfo":{"status":"ok","timestamp":1745446931521,"user_tz":360,"elapsed":78,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+----------+-----------------+---------+\n","|order_id|product_id|add_to_cart_order|reordered|\n","+--------+----------+-----------------+---------+\n","|2       |33120     |1                |1        |\n","|2       |28985     |2                |1        |\n","|2       |9327      |3                |0        |\n","|2       |45918     |4                |1        |\n","|2       |30035     |5                |0        |\n","+--------+----------+-----------------+---------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["#Combine train + prior order‑items\n","order_products = train_op.unionByName(prior_op)"],"metadata":{"id":"8M5hJ8op2P4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["order_products.show(5, truncate=False)\n","print((\"order_products rows\", order_products.count()), (\"columns\", len(order_products.columns)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvKzxqIX2Thh","outputId":"0958e9a5-3805-4fe9-c404-9fc1d9057474","executionInfo":{"status":"ok","timestamp":1745446977770,"user_tz":360,"elapsed":557,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+----------+-----------------+---------+\n","|order_id|product_id|add_to_cart_order|reordered|\n","+--------+----------+-----------------+---------+\n","|1       |49302     |1                |1        |\n","|1       |11109     |2                |1        |\n","|1       |10246     |3                |0        |\n","|1       |49683     |4                |0        |\n","|1       |43633     |5                |1        |\n","+--------+----------+-----------------+---------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["# Enrich products with aisle & department names\n","product_meta = (\n","    products\n","        .join(aisles, on=\"aisle_id\", how=\"inner\")\n","        .join(depts,  on=\"department_id\", how=\"inner\")\n",")\n"],"metadata":{"id":"1fTjZMBC2VuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Join order‑items → product metadata → orders\n","master = (\n","    order_products\n","        .join(product_meta, on=\"product_id\", how=\"inner\")\n","        .join(orders,       on=\"order_id\",   how=\"inner\")\n",")"],"metadata":{"id":"9GluXVRC2ax8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the number of rows and columns in the master DataFrame\n","num_rows = master.count()\n","num_cols = len(master.columns)\n","\n","# Print the shape as a tuple\n","print((num_rows, num_cols))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ehm2PWnk5c0K","outputId":"b0a62e3c-8cfe-4422-f722-5ce527b5e4de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(33819103, 15)\n"]}]},{"cell_type":"code","source":["master.show(5, truncate=True)\n","print((\"master rows\", master.count()), (\"columns\", len(master.columns)))"],"metadata":{"id":"HoiCg7F_7ouE","colab":{"base_uri":"https://localhost:8080/","height":991},"executionInfo":{"status":"error","timestamp":1745447170018,"user_tz":360,"elapsed":75550,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}},"outputId":"b383100e-a74a-4263-e54e-1301494c8123"},"execution_count":null,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o114.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 48) (d323adf04bce executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b13884f13ec2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"master rows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 48) (d323adf04bce executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}]},{"cell_type":"code","source":["type(master.select('product_name'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":186},"id":"LamyCLqqwzoO","executionInfo":{"status":"ok","timestamp":1745447473796,"user_tz":360,"elapsed":88,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}},"outputId":"22ce2a16-85a4-49e1-923c-582abbf2e37a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n","\n",".. versionadded:: 1.3.0\n","\n",".. versionchanged:: 3.4.0\n","    Supports Spark Connect.\n","\n","Examples\n","--------\n","A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n","and can be created using various functions in :class:`SparkSession`:\n","\n","&gt;&gt;&gt; people = spark.createDataFrame([\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n","...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n","...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n","...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n","... ])\n","\n","Once created, it can be manipulated using the various domain-specific-language\n","(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n","\n","To select a column from the :class:`DataFrame`, use the apply method:\n","\n","&gt;&gt;&gt; age_col = people.age\n","\n","A more concrete example:\n","\n","&gt;&gt;&gt; # To create DataFrame using SparkSession\n","... department = spark.createDataFrame([\n","...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n","...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n","...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n","... ])\n","\n","&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n","...     department, people.deptId == department.id).groupBy(\n","...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n","+-------+------+-----------+--------+\n","|   name|gender|avg(salary)|max(age)|\n","+-------+------+-----------+--------+\n","|     ML|     F|      150.0|      60|\n","|PySpark|     M|       75.0|      50|\n","+-------+------+-----------+--------+\n","\n","Notes\n","-----\n","A DataFrame should only be created as described above. It should not be directly\n","created via using the constructor.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 80);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["master.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZXoqcym2jxu","outputId":"7d78e34f-81d1-4beb-f1f2-47cc869e63c3","executionInfo":{"status":"ok","timestamp":1745447212630,"user_tz":360,"elapsed":12,"user":{"displayName":"Rafael Cintron","userId":"09346524679635125204"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- order_id: integer (nullable = true)\n"," |-- product_id: integer (nullable = true)\n"," |-- add_to_cart_order: integer (nullable = true)\n"," |-- reordered: integer (nullable = true)\n"," |-- department_id: string (nullable = true)\n"," |-- aisle_id: string (nullable = true)\n"," |-- product_name: string (nullable = true)\n"," |-- aisle: string (nullable = true)\n"," |-- department: string (nullable = true)\n"," |-- user_id: integer (nullable = true)\n"," |-- eval_set: string (nullable = true)\n"," |-- order_number: integer (nullable = true)\n"," |-- order_dow: integer (nullable = true)\n"," |-- order_hour_of_day: integer (nullable = true)\n"," |-- days_since_prior_order: double (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import rand\n","TARGET_SAMPLE = 2_500_000\n","RANDOM_SEED   = 42\n","\n","\n","# Shuffle the DataFrame randomly\n","shuffled_df = master.orderBy(rand(seed=RANDOM_SEED))\n","\n","#Get the first 2.5 million rows (no replacement because we just shuffled)\n","sampled_df = shuffled_df.limit(TARGET_SAMPLE)\n","sampled_df.cache()\n","\n"],"metadata":{"id":"H56f-9nc7vwr"},"execution_count":null,"outputs":[]}]}